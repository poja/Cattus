%YAML 1.2
---
# The game type
# tictactoe, hex5, hex7, hex9, hex11, chess
game: "tictactoe"

# Path to working area where models, games and metrics are saved
# {CATTUS_TOP}, {GAME_NUM} can be used
working_area: "{CATTUS_TOP}/workarea/{GAME_NAME}"
mcts:
    # Number of simulation performed at each search
    # suggested value: 600-1400
    sim_num: 600

    # Explore vs exploit factor
    # suggested value: sqrt(2)
    explore_factor: 1.41421

    # Dirichlet noise added to root level prior edges scores
    # suggested value: (0.03, 0.25)
    prior_noise_alpha: 0.03
    prior_noise_epsilon: 0.25

    # Network output cache size
    cache_size: 1000000

self_play:
    # Number of iterations of (self-play, train, compare)
    # suggested value: > 100
    iterations: 100

    # Softmax temperature
    # suggested value: 1.0 for the first 30 moves, 0.0 for the reset of the game
    temperature_policy:
        - [30,    1.0]
        - [       0.0]

    # Number of games played in each self play iteration
    # must be multiple of 2
    # suggested value: > 100
    games_num: 100

    # Number of threads used during self play
    threads: 8

model:
    # Path to the base model for the training process
    # [none] create new model
    # [latest] use model with latest timestamp
    base: "[none]"

    # Type of network ("ConvNetV1", "simple_two_headed")
    type: "ConvNetV1"

    # Number of residual blocks, not including the first conv layer
    # suggested value: 7-39
    residual_block_num: 7

    # Number of filters in the conv layers within each residual block
    # suggested value: 32-256
    # WARN 32 seems to cause error
    residual_filter_num: 16

    # Number of channels in the conv layer of the value head
    # suggested value: 8-32
    value_head_conv_output_channels_num: 8

    # Number of channels in the conv layer of the policy head
    # suggested value: 8-32
    policy_head_conv_output_channels_num: 8

    # L2 regularization factor
    # suggested value: 10^-4
    l2reg: 0.00005

training:
    # The number of latest entries considered as training data
    # suggested value: (1-16) * iteration_data_entries
    latest_data_entries: 262144

    # Number of entries used in a single training iteration, uniformly from latest entries3
    # suggested value: (1-16) * (self play games_num) * (average positions per game)
    iteration_data_entries: 32768

    # Batch size
    # suggested value: 64-256
    batch_size: 128

    learning_rate:
        - [262144, 0.01]
        - [786432, 0.001]
        - [       0.0001]

model_compare:
    # Softmax temperature used during model comparison
    # Use 0 for best model performance and better comparison
    temperature_policy:
        - [       0.0]

    # Number of games played to compare two models
    # must be multiple of 2
    # suggested value: > 30
    games_num: 20

    # Comparison winning ratio at which the best model is switched
    switching_winning_threshold: 0.55

    # Comparison losing ratio at which a warning will be printed
    warning_losing_threshold: 0.55

    # Number of threads used to during model comparison
    threads: 8

# Runs on CPU if true, else on GPU
cpu: true

# Use debug executable if true, else uses release executables
debug: false

# If true, training data will be reused from multiple runs, always with preference to newer data
# Suggested value: true
use_train_data_across_runs: true
